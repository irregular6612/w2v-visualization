{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/tiktoken/load.py:154\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     token, rank \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    155\u001b[0m     ret[base64\u001b[38;5;241m.\u001b[39mb64decode(token)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rank)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1636\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1632\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1533\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1533\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1535\u001b[0m         [\n\u001b[1;32m   1536\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1537\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1538\u001b[0m         ]\n\u001b[1;32m   1539\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1526\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1526\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1527\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1502\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1500\u001b[0m     )\n\u001b[0;32m-> 1502\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m byte_encoder \u001b[38;5;241m=\u001b[39m bytes_to_unicode()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/tiktoken/load.py:157\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken_bpe_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mValueError\u001b[0m: Error parsing line b'\\x0e' in /Users/bagjuhyeon/.cache/huggingface/hub/models--skt--kobert-base-v1/snapshots/a9f5849fce18fb088f0cd0f9b29ec3f756958464/spiece.model",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskt/kobert-base-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 토크나이저와 모델 로드\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 모델 정보 출력\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         )\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2030\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2272\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2272\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2274\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2277\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/models/xlnet/tokenization_xlnet_fast.py:132\u001b[0m, in \u001b[0;36mXLNetTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, remove_space, keep_accents, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    114\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m ):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_accents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_accents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_token_type_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:138\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 138\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/bert/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1638\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1634\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1635\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1636\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1639\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1642\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BErtTokenizer, AutoModel\n",
    "\n",
    "# KoBERT 모델 로드 (SKT에서 제공하는 모델)\n",
    "model_name = \"skt/kobert-base-v1\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"모델 이름: {model_name}\")\n",
    "print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")\n",
    "\n",
    "# 간단한 텍스트 처리 예제\n",
    "text = \"안녕하세요, KoBERT 모델을 테스트하고 있습니다.\"\n",
    "\n",
    "# 토큰화\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"\\n토큰화 결과:\")\n",
    "print(f\"입력 ID: {inputs['input_ids']}\")\n",
    "print(f\"토큰 타입 ID: {inputs['token_type_ids']}\")\n",
    "print(f\"어텐션 마스크: {inputs['attention_mask']}\")\n",
    "\n",
    "# 토큰 ID를 다시 토큰으로 변환\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(f\"\\n토큰 목록: {tokens}\")\n",
    "\n",
    "# 모델 실행 (임베딩 추출)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 출력 확인\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(f\"\\n마지막 은닉 상태 크기: {last_hidden_states.shape}\")\n",
    "print(f\"첫 번째 토큰([CLS]) 임베딩 일부: {last_hidden_states[0, 0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1', sp_model_kwargs={'nbest_size': -1, 'alpha': 0.6, 'enable_sampling': True})\n",
    "\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "text = \"한국어 모델을 공유합니다.\"\n",
    "inputs = tokenizer.batch_encode_plus([text])\n",
    "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "              attention_mask = torch.tensor(inputs['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"한국어 모델을 공유합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1554, -0.0150,  0.3673,  ..., -0.0094,  0.1067,  0.0844],\n",
       "         [ 0.1229, -0.3236, -0.0669,  ..., -0.4487, -0.1753, -0.2302],\n",
       "         [ 0.1008, -0.3885, -0.1219,  ..., -0.2129, -0.0330, -0.1708],\n",
       "         ...,\n",
       "         [-0.0700,  0.1174, -0.1385,  ..., -0.0272,  0.5141, -0.0093],\n",
       "         [-0.0605, -0.2776,  0.4285,  ..., -0.2879,  0.5493,  0.0797],\n",
       "         [-0.1537, -0.1819, -0.1994,  ..., -0.2862,  0.0133,  0.0783]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.5197e-02,  1.2525e-02, -2.0906e-02, -4.9571e-02, -9.7731e-01,\n",
       "          9.9156e-01, -6.7530e-01,  5.5346e-02,  2.3231e-02,  4.5099e-03,\n",
       "         -5.1811e-01, -3.6802e-02, -4.0091e-02, -7.1836e-02, -2.3317e-02,\n",
       "          8.3534e-01, -9.9642e-01,  5.2746e-02,  2.5372e-03, -8.5501e-02,\n",
       "          1.1586e-01, -4.6590e-02, -2.1022e-02,  5.6610e-01,  3.4027e-02,\n",
       "          7.5981e-01, -9.3067e-01, -6.7136e-02, -9.6185e-01, -1.7121e-01,\n",
       "          9.5807e-01,  5.7800e-01, -1.9471e-02, -4.1816e-02, -9.3780e-01,\n",
       "          4.9917e-02, -5.7411e-03, -2.2729e-02, -9.9425e-01, -6.9237e-02,\n",
       "          6.8455e-04,  3.4909e-02, -6.2844e-03,  9.9442e-01, -9.9899e-01,\n",
       "          4.0310e-02, -5.3533e-01, -2.2571e-02, -9.9929e-01,  9.8811e-01,\n",
       "         -1.2751e-01, -9.9998e-01, -9.9651e-01,  5.4740e-03, -3.5815e-02,\n",
       "         -8.2744e-01,  3.8543e-02,  8.0260e-01,  2.5858e-02, -3.3279e-02,\n",
       "          3.3252e-02, -1.3364e-02,  9.6320e-01, -9.9580e-01,  2.3712e-02,\n",
       "         -9.8590e-03, -2.1819e-02,  5.7131e-02,  1.9689e-02, -3.2285e-02,\n",
       "          9.7799e-01, -9.0611e-02,  1.7279e-01, -3.2946e-02, -5.7409e-01,\n",
       "          5.7556e-05,  3.2987e-02, -2.1082e-02,  1.1405e-01, -7.1178e-02,\n",
       "          2.7412e-02, -9.9241e-01, -6.7661e-01,  2.8413e-01,  2.0271e-03,\n",
       "         -3.9799e-02,  1.1175e-01, -6.5503e-02,  9.9622e-01, -8.3377e-01,\n",
       "          4.3311e-02, -2.6061e-02, -3.0420e-02,  4.8004e-02, -9.8523e-01,\n",
       "         -4.1780e-03,  1.7801e-02, -5.3233e-02,  1.1994e-02,  8.4690e-01,\n",
       "          3.1516e-03, -9.9731e-03,  1.0782e-02, -7.0629e-03, -9.6009e-01,\n",
       "         -5.2606e-02, -3.7220e-03, -3.6499e-02, -1.3431e-03,  9.6021e-03,\n",
       "          1.2540e-02, -4.1232e-02,  9.6691e-01,  1.4780e-02,  2.9358e-01,\n",
       "         -1.1180e-02, -2.6618e-03, -4.7589e-03,  3.1942e-02,  3.5106e-02,\n",
       "          8.3404e-02, -6.5394e-02, -9.9862e-01,  9.5117e-01, -9.9999e-01,\n",
       "          1.0240e-03, -8.7128e-01, -7.1489e-02, -3.0123e-02, -9.1399e-02,\n",
       "         -1.1826e-02,  2.4025e-02,  1.5446e-02,  8.2456e-02, -1.4861e-02,\n",
       "         -5.0144e-02, -8.9484e-01,  5.6974e-03,  4.4966e-02, -5.3344e-02,\n",
       "         -3.0326e-03,  6.7165e-02,  1.0825e-01,  8.2401e-01, -1.8424e-02,\n",
       "          6.7906e-02,  3.0821e-02,  2.8489e-02,  3.5309e-02,  1.5438e-02,\n",
       "         -9.8715e-01, -6.0995e-02, -9.1701e-01, -3.8465e-02,  1.2546e-02,\n",
       "          5.3762e-02,  3.0931e-02, -7.0557e-03,  1.4816e-03, -9.4359e-01,\n",
       "          1.0758e-02,  5.6370e-02,  3.6725e-02,  9.9708e-01,  3.1692e-02,\n",
       "          1.1595e-03, -1.4543e-02,  4.3772e-01, -2.6825e-02, -4.5654e-02,\n",
       "         -5.8780e-02, -5.8295e-02, -9.9117e-01, -6.8371e-01,  7.8358e-02,\n",
       "          9.9997e-01,  2.8381e-02, -9.5989e-01, -2.5401e-03, -3.2165e-02,\n",
       "          9.8768e-02,  9.9097e-01,  8.6353e-02, -9.9999e-01,  7.0322e-03,\n",
       "         -1.0353e-02,  6.1644e-02, -1.5536e-02,  5.6378e-02,  4.5823e-01,\n",
       "         -3.2974e-02, -9.2151e-03, -8.3104e-01, -1.9364e-02,  7.2393e-03,\n",
       "         -1.2476e-02,  2.1621e-01, -1.8092e-03, -3.4680e-02,  1.2223e-02,\n",
       "         -2.0087e-02,  6.7524e-01,  2.7462e-02, -1.7318e-02,  9.6151e-01,\n",
       "          4.1258e-03, -8.5152e-02,  3.8312e-02, -5.9093e-01,  9.5981e-01,\n",
       "          7.2530e-03,  3.1033e-02,  9.5110e-01,  9.1042e-01,  1.1993e-01,\n",
       "          2.7889e-03, -1.5895e-02,  9.7395e-01,  9.4464e-01, -9.9836e-01,\n",
       "         -9.5188e-01,  1.6848e-02,  4.1663e-03,  4.0427e-02, -4.4812e-03,\n",
       "         -3.1519e-02,  9.8529e-01, -9.9806e-01, -2.1533e-02, -7.7940e-02,\n",
       "         -9.9560e-01, -2.1861e-04,  5.3601e-03, -2.5698e-01,  2.7291e-01,\n",
       "         -2.3914e-01,  5.3425e-01,  3.9198e-03,  3.2111e-03, -1.2114e-02,\n",
       "          9.3507e-01, -2.4086e-02,  9.3231e-01, -3.6020e-02,  7.5740e-01,\n",
       "          3.5909e-02, -2.5440e-02,  9.9753e-01,  1.2859e-01,  9.9417e-01,\n",
       "         -1.8082e-02, -3.7021e-02,  1.0731e-02, -9.5282e-03, -6.0838e-02,\n",
       "         -7.9809e-02,  9.9791e-01, -1.8524e-03,  9.9517e-01, -9.5933e-01,\n",
       "          5.3300e-02,  5.2023e-02,  6.4620e-03, -1.2404e-02,  2.6209e-01,\n",
       "          4.0104e-02, -2.9280e-03, -5.7839e-01, -9.6004e-01,  2.9432e-02,\n",
       "          5.6997e-02, -5.3312e-02,  4.8071e-03,  1.6104e-01,  8.4932e-01,\n",
       "         -9.4791e-01,  2.0395e-02, -9.2237e-03, -1.9253e-03,  3.9376e-02,\n",
       "          1.7074e-02,  4.0028e-01, -9.8209e-01,  3.5404e-02,  2.1781e-03,\n",
       "         -7.7996e-03, -1.7152e-02, -9.3239e-02,  2.0302e-02,  2.3005e-02,\n",
       "          9.8101e-01, -3.3571e-02,  7.5093e-01, -3.5337e-02,  2.1599e-02,\n",
       "         -5.6106e-02, -6.3388e-01,  9.9726e-01,  1.7766e-01,  3.1342e-02,\n",
       "         -5.6624e-02,  9.2133e-02, -4.1115e-02, -4.9083e-02,  9.9997e-01,\n",
       "          8.4378e-02, -6.6566e-01,  1.6982e-02,  2.1651e-02, -7.0314e-02,\n",
       "          6.4436e-02,  7.7849e-02,  5.7229e-02,  9.6834e-01,  9.9999e-01,\n",
       "         -3.0596e-02, -2.1303e-03, -3.0537e-02, -3.0713e-02,  6.3320e-02,\n",
       "         -5.1865e-02,  1.6236e-04,  3.4152e-02, -2.7721e-02, -5.6890e-02,\n",
       "         -9.9176e-02, -9.9096e-01,  2.8188e-02, -2.2625e-03, -1.5026e-02,\n",
       "         -4.5539e-02, -2.5171e-02, -9.5657e-01, -1.4087e-01, -9.6047e-01,\n",
       "         -2.2658e-02,  1.6845e-02, -9.2824e-02, -6.2046e-02,  9.5685e-01,\n",
       "          3.8434e-02,  2.0035e-02,  8.2179e-01, -8.4109e-03, -9.8684e-01,\n",
       "         -9.9978e-01,  8.8911e-03,  8.5371e-01, -5.6934e-02,  5.8383e-02,\n",
       "         -9.9471e-02, -8.8987e-02, -2.9000e-01,  4.4525e-02, -4.8241e-02,\n",
       "         -5.0006e-03, -7.3095e-01,  9.9998e-01,  9.2479e-03,  1.7111e-02,\n",
       "          1.9900e-02,  5.9194e-02, -5.7849e-01, -9.9301e-01,  2.2823e-02,\n",
       "         -1.4389e-02, -9.9871e-01, -5.9338e-04,  3.1265e-02,  2.1788e-03,\n",
       "         -9.3780e-01,  5.1157e-03, -5.8728e-02, -4.1728e-02,  1.2126e-01,\n",
       "          4.8314e-02, -9.2558e-01,  4.7209e-01, -1.3779e-02, -6.0363e-03,\n",
       "         -9.9998e-01,  5.7253e-03,  9.7654e-01, -5.1506e-02,  1.5155e-02,\n",
       "          4.1717e-02, -2.8035e-02, -9.1637e-03,  3.5195e-03, -6.1744e-02,\n",
       "         -9.5536e-01,  9.9996e-01, -3.6490e-01, -2.3475e-01,  7.5843e-02,\n",
       "         -1.7307e-01,  5.8301e-02, -9.4847e-03,  1.3548e-02,  1.9693e-02,\n",
       "         -9.4166e-03, -8.7349e-02,  1.1062e-01,  9.4669e-01, -7.8666e-01,\n",
       "          9.8472e-01,  6.9207e-02, -6.6394e-02, -4.1203e-01, -2.8216e-02,\n",
       "          1.8331e-02, -9.9501e-01, -9.6914e-01, -3.6959e-02, -2.0501e-02,\n",
       "         -5.5200e-01, -9.6724e-03, -3.2003e-02, -5.2948e-02, -3.2587e-02,\n",
       "         -3.5871e-02, -5.7683e-01, -1.7646e-03, -9.2759e-01,  8.5869e-01,\n",
       "         -3.4394e-02,  2.0066e-02, -5.8168e-01,  8.4513e-02, -6.3545e-02,\n",
       "         -4.9387e-01, -2.0327e-02, -5.5759e-01, -6.8314e-01, -3.4532e-01,\n",
       "         -5.0017e-02,  9.3841e-01, -2.8326e-01,  8.4987e-01,  9.3719e-01,\n",
       "          6.3603e-01,  1.9851e-03, -3.3372e-02,  9.9999e-01, -7.0330e-01,\n",
       "          4.8064e-02,  5.7000e-02,  6.3821e-01,  8.9572e-01, -1.8793e-02,\n",
       "         -9.5766e-01,  7.3250e-03, -9.7303e-01,  5.7000e-02, -4.8620e-02,\n",
       "          4.8304e-01,  3.9378e-02, -3.8751e-01,  1.3293e-03,  8.2993e-01,\n",
       "          2.3834e-02, -4.2979e-03, -4.3612e-02,  2.3217e-02, -1.9303e-02,\n",
       "          9.6390e-01,  2.4040e-02, -9.4583e-01, -9.4278e-02, -9.8666e-01,\n",
       "          2.4492e-02,  6.3368e-02,  7.4936e-03,  5.4627e-01, -1.5069e-03,\n",
       "         -4.0154e-01, -4.0156e-03, -4.0067e-02,  8.2598e-01,  9.4181e-02,\n",
       "         -9.9998e-01, -9.8626e-01, -9.3065e-01,  3.3448e-03,  2.6259e-01,\n",
       "         -9.8344e-01,  8.9739e-02,  7.5558e-02, -1.1075e-01, -9.9954e-01,\n",
       "         -5.7097e-02,  5.7236e-02, -5.3608e-02,  1.0742e-01, -9.3713e-01,\n",
       "         -7.5513e-03,  4.3232e-01, -2.4343e-03,  3.1716e-03, -1.4683e-01,\n",
       "         -9.9099e-01,  1.3144e-02, -3.1915e-02, -5.2123e-01, -2.4655e-02,\n",
       "          4.2343e-01,  2.3706e-01, -8.4742e-01,  9.9224e-01, -4.3127e-02,\n",
       "         -7.2845e-02, -4.3132e-01,  9.7068e-02,  7.9606e-03, -7.9714e-01,\n",
       "          9.2455e-01,  6.5169e-02, -1.9123e-02, -5.2973e-02, -9.3342e-02,\n",
       "         -3.4624e-02,  8.1650e-02, -1.8784e-02, -8.0761e-01, -7.1122e-03,\n",
       "          8.5199e-01,  5.0713e-02,  3.3229e-04,  6.9106e-02,  4.2433e-02,\n",
       "         -9.8644e-01,  3.3141e-03, -9.9184e-01, -2.9564e-02,  8.9984e-01,\n",
       "          7.2060e-01, -7.9699e-02,  1.2134e-02,  1.0017e-03, -9.9989e-01,\n",
       "         -2.9644e-03, -1.0000e+00,  7.8761e-02,  6.1437e-01, -3.4938e-02,\n",
       "         -2.3349e-02,  2.1531e-02,  1.8100e-02, -8.7273e-01, -9.2702e-01,\n",
       "         -4.7151e-02, -9.9999e-01, -9.8995e-01, -8.0021e-01, -9.4308e-02,\n",
       "          2.3066e-01,  4.6256e-02,  5.7805e-01, -1.8897e-02, -3.0696e-02,\n",
       "         -5.0692e-01, -6.9982e-02,  4.6907e-02,  3.9684e-02, -9.4130e-01,\n",
       "         -4.4868e-02,  2.5180e-02, -2.5699e-02,  2.3236e-02, -5.0229e-01,\n",
       "          7.0390e-03, -4.7195e-02,  2.6988e-02, -9.9311e-01,  2.9732e-02,\n",
       "          1.0654e-02,  3.9880e-02,  2.5688e-01, -7.8164e-02,  3.3402e-03,\n",
       "          6.0766e-02, -9.6415e-01,  2.8404e-04, -3.7492e-02,  8.8840e-01,\n",
       "         -9.1813e-03, -3.4287e-03, -9.6691e-01, -3.5383e-02,  7.5131e-01,\n",
       "          6.9293e-01,  6.9224e-01,  9.9303e-01,  7.8836e-02,  6.4091e-01,\n",
       "         -7.7383e-03, -4.3151e-01,  8.0024e-02,  9.5829e-01, -2.8434e-02,\n",
       "          1.5491e-03,  2.0705e-02, -9.8741e-01,  5.4546e-02, -9.9990e-01,\n",
       "          5.8355e-02, -9.8138e-01, -9.8336e-01, -1.3431e-03,  2.0571e-02,\n",
       "         -8.8442e-01, -7.1332e-01,  2.6191e-02, -1.8135e-02,  5.9191e-03,\n",
       "          1.6197e-02,  7.1325e-03, -9.5013e-01, -1.3307e-02,  5.8956e-01,\n",
       "         -1.3086e-02, -1.3999e-02,  4.2700e-02, -6.2579e-02, -2.7303e-02,\n",
       "          9.4153e-03, -1.4350e-02,  9.6016e-01,  4.4394e-01, -9.5806e-01,\n",
       "          8.1432e-01, -9.9057e-03, -9.5997e-01, -4.9583e-02,  3.1852e-02,\n",
       "         -5.6342e-03,  9.7473e-01,  3.0388e-02,  1.0000e+00,  9.9039e-01,\n",
       "         -9.6436e-01,  6.2123e-01,  1.2589e-03,  6.3712e-01, -5.6715e-02,\n",
       "         -9.5321e-01, -5.4790e-02, -9.4971e-01, -4.0235e-02,  6.9230e-03,\n",
       "          4.2838e-02, -3.1996e-01, -9.9841e-01,  2.5414e-02, -2.3247e-02,\n",
       "         -1.0253e-01, -9.1436e-01, -2.9565e-02, -9.9994e-01,  2.1628e-02,\n",
       "          9.8026e-01,  9.2917e-01, -3.8466e-02, -1.7672e-02,  9.9858e-01,\n",
       "          9.0906e-01, -9.3093e-01,  5.4480e-03,  3.8040e-02,  1.1974e-02,\n",
       "         -9.8447e-01, -1.9743e-02, -3.3283e-01, -4.8351e-04,  7.2251e-02,\n",
       "          1.2176e-02,  1.0974e-02,  5.4018e-01, -1.2868e-01, -5.4206e-02,\n",
       "          5.7446e-03,  9.0365e-01,  1.2500e-02, -9.6540e-02,  3.5238e-02,\n",
       "         -7.0963e-01,  5.5002e-02,  9.9998e-01,  8.4911e-01,  9.4085e-01,\n",
       "          9.9761e-01, -6.3110e-03, -4.0790e-02,  9.3981e-01,  6.3795e-01,\n",
       "          3.6250e-02, -7.2845e-02, -9.9647e-01, -1.8132e-02,  2.9783e-01,\n",
       "          9.6961e-01,  3.8222e-01, -9.9998e-01,  8.3317e-03,  9.8380e-01,\n",
       "          9.7967e-01, -6.5250e-01,  9.3331e-01, -9.8891e-01, -6.1939e-01,\n",
       "          3.6034e-02,  1.4953e-01, -7.4738e-01, -5.3544e-02,  4.3884e-02,\n",
       "          9.9999e-01,  1.5039e-01,  1.6797e-02,  2.6905e-02,  1.9737e-02,\n",
       "         -6.8264e-02, -4.8807e-02, -2.9707e-02,  9.4600e-01, -1.3704e-02,\n",
       "          3.7268e-03, -1.1899e-02, -9.9979e-01, -3.3232e-01, -2.0500e-01,\n",
       "         -1.3862e-02, -1.1781e-02, -9.9879e-01,  2.6497e-02, -4.6408e-02,\n",
       "         -7.0235e-04, -2.4122e-01,  2.3335e-03, -2.4632e-02,  3.5002e-01,\n",
       "         -9.9635e-03, -2.0773e-02, -1.5709e-02,  7.4071e-02,  5.5607e-02,\n",
       "          9.7022e-01, -6.7267e-01,  1.5011e-02,  1.9265e-01,  9.9499e-01,\n",
       "          1.2096e-02,  2.6780e-02, -5.0804e-01, -9.8375e-01, -9.6202e-01,\n",
       "          9.8026e-01,  3.7138e-02, -7.5124e-02, -3.8577e-02, -7.8318e-01,\n",
       "          3.1657e-01,  9.4517e-01, -7.7084e-01, -3.6669e-02, -9.9821e-01,\n",
       "          9.7941e-02, -5.3265e-03,  1.8041e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 간 코사인 유사도:\n",
      "문장 1과 문장 2의 유사도: 0.9400\n",
      "  - '나는 너를 좋아해.'\n",
      "  - '너는 나를 좋아해.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 문장 임베딩 추출 함수\n",
    "def get_sentence_embedding(text, model, tokenizer):\n",
    "    # 토큰화\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # 모델 실행\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # [CLS] 토큰의 마지막 은닉 상태 사용 (문장 임베딩)\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return sentence_embedding[0]\n",
    "\n",
    "# 테스트할 한국어 문장들\n",
    "sentences = [\n",
    "    \"오늘 날씨가 정말 좋네요.\",\n",
    "    \"날씨가 참 화창한 하루입니다.\",\n",
    "    \"인공지능 기술이 빠르게 발전하고 있습니다.\",\n",
    "    \"딥러닝 모델은 대량의 데이터로 학습됩니다.\",\n",
    "    \"한국어 자연어 처리는 중요한 연구 분야입니다.\"\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    '계란이 맛있다.',\n",
    "    '닭이 맛있다.',\n",
    "    '치킨이 맛있다.',\n",
    "    '후라이가 맛있다.'\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    '나는 너를 좋아해.',\n",
    "    '너는 나를 좋아해.'\n",
    "]\n",
    "\n",
    "# 각 문장의 임베딩 추출\n",
    "embeddings = np.array([get_sentence_embedding(sentence, model, tokenizer) for sentence in sentences])\n",
    "\n",
    "# 문장 간 코사인 유사도 계산\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# 유사도 결과 출력\n",
    "print(\"문장 간 코사인 유사도:\")\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        print(f\"문장 {i+1}과 문장 {j+1}의 유사도: {similarity_matrix[i][j]:.4f}\")\n",
    "        print(f\"  - '{sentences[i]}'\")\n",
    "        print(f\"  - '{sentences[j]}'\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
